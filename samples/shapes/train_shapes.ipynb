{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\_me\\Code\\2_github\\3_zglezgle\\AI-Generic\\Mask-R-CNN\\Mask_RCNN\\mrcnn\\model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\_me\\Code\\2_github\\3_zglezgle\\AI-Generic\\Mask-R-CNN\\Mask_RCNN\\mrcnn\\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\_me\\Code\\2_github\\3_zglezgle\\AI-Generic\\Mask-R-CNN\\Mask_RCNN\\mrcnn\\model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\_me\\Code\\2_github\\3_zglezgle\\AI-Generic\\Mask-R-CNN\\Mask_RCNN\\logs\\shapes20200514T1229\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n",
      "C:\\Users\\pw33095\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\skimage\\transform\\_warps.py:830: FutureWarning: Input image dtype is bool. Interpolation is not defined with bool data type. Please set order to 0 or explicitely cast input image to another data type. Starting from version 0.19 a ValueError will be raised instead of this warning.\n",
      "  order = _validate_interpolation_order(image.dtype, order)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[256,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mrcnn_mask_bn4/FusedBatchNormV3-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/add_5/_4873]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[256,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mrcnn_mask_bn4/FusedBatchNormV3-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             layers='heads')\n\u001b[0m",
      "\u001b[1;32mC:\\_me\\Code\\2_github\\3_zglezgle\\AI-Generic\\Mask-R-CNN\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   2372\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2373\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2374\u001b[1;33m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2375\u001b[0m         )\n\u001b[0;32m   2376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MaskRCNN-v1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[256,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mrcnn_mask_bn4/FusedBatchNormV3-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/add_5/_4873]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[256,14,14,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mrcnn_mask_bn4/FusedBatchNormV3-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
